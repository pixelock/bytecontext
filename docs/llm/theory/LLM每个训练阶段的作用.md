一个 LLM 从零开始构建可以分为三个训练阶段, 分别是:

- **基座模型预训练**: 预训练的目的是预测下一个 token
- **SFT**: 也即 Instruction-tuning
- **RLHF**: 模型对输入的 instruction 生成答案, 并评估生成的答案好坏, 优化模型参数

# 不同训练阶段的不同作用

总的来说, 三个不同的训练阶段发挥的作用可以总结如下:

- 预训练: 学习并存储各种知识
- SFT: **激发和引导模型输出知识**
- RLHF: 进一步对齐人类价值观并减少幻觉

## 预训练

训练的目的是预测下一个 token, 通过预训练得到的模型, 可以为潜在的下一个 token 选项分配一个概率, 通过这种方式, 模型获得了语言的内部联系, 又可以称为知识.

在预训练之后, 模型就已经有很强的文本生成能力了. 但它的训练语料和训练方式, 决定了它可以为**给定的文本前缀提供自然的续写**, 但是**不擅长对话交流, 或者说不擅长回答问题**. 例如, 当给定一个输入问题时, 它可能会对这个问题进行续写, 或者生成该问题的不同表达方式, 或者是产生一系列额外的问题.

我们可以通过设计 Prompt, 来让模型为我们解决问题, 但模型往往会对这些 Prompt 非常敏感, 需要做 Prompt Engineering 来精心地设计 prompt, 而 prompt 上细微的差异, 就会导致输出结果(或者成为回答问题)发生很大的变化. 这不是一种高效且稳定的解决方案.

我们需要的是模型可以根据用户的 query, 按照某种特定的 style 来回答, 因此我们需要通过进一步的训练, 来**引导模型朝着这种 style 发展**.

## SFT / Instruction-tuning

上面所说的指导我们称之为 SFT: 继续训练预训练得到的模型, 使其回答问题的表现如同我们期待的那样.

这个阶段用来训练的文本是 Instruction 形式. 例如 Instruction 是一个问题, 然后是这个问题的答案; 或者是一个 Instruction 代表任务, 一段文本作为输入, 然后是根据这段输入回答的答案.

训练方式仍是预测下一个 token, 但是在指令数据集上, 模型学习的是**响应指令**. 通过训练将这种根据 instruction 输出正确的 response 的行为泛化到其他不同的 instruction 上去.

## RLHF

强化学习要做的, 是完成 **credit assignment**, 即对答案的好坏进行评价, 通过 reward model, 对模型生成的答案进行评价(分配一个分数, 或者比较两个答案的好坏).

SFT 已经可以让模型按照我们希望的方式生成答案, 但还需要强化学习训练来保证模型输出高质量的答案(多样性, 无害, 符合价值观等).

高质量的 LLM 必须 RLHF 这一步的原因, 主要有:

SFT 这些监督学习**只允许正反馈**(我们向模型展示一系列问题及其正确答案), 而**强化学习允许负反馈**(模型被允许生成一个答案并得到一个反馈说这是不正确的), 而**负反馈的作用更强大**. 这是因为对于监督学习, 错误的示例可能会误导模型学习到错误的假设; 而作为一个学生如果被允许形成自己的假设, 并问教师是否正确, 这种情况下训练的模型要强大得多.

另外, 通过强化学习可以减轻幻觉. LLM 的核心是**鼓励模型根据它的内部知识来回答**, 但我们不知道模型学习到了哪些知识. 在监督训练中, 我们向模型提出问题并提供正确答案, 然后训练模型学习提供的答案, 这可能会存在两种情况:

- 模型拥有相关的知识. 在这种情况下, 监督训练正确地推动它把答案与问题联系起来, 希望推动它在未来执行类似的步骤来回答类似的问题, 这是期望的行为
- 模型没有相关知识. 在这种情况下, 监督训练会促使模型无论如何将答案与问题相关联. 有两种选择: 1. 它可能会促使模型记住这个特定的问答对, 这没有害处, 但也不是很有效, 因为我们的目标是让模型泛化并学会回答任何问题, 而不仅仅是指令训练数据中的问题; 2. 如果我们成功地训练模型在这些情况下进行泛化, 那么实际上就是在教模型编造东西, 监督训练在鼓励模型说谎, 这也是我们诟病的**模型幻觉**的最大来源

因此如果只使用监督训练, 它会教导模型说谎, 我们不能使用纯粹的监督学习来推动模型产生真实的答案. 与监督学习相比, 强化学习不会主动鼓励模型说谎, 即使模型最初确实猜对了一些答案并错误地学习了编造行为, 但从长远来看, 对于编造的答案它会得到不好的分数, 并学会采用依赖于其内部知识的策略, 或者干脆弃权.

---

# 参考资料

- [Reinforcement Learning for Language Models](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81)
- [RL对于LLM到底意味着什么？听听ChatGPT作者John Schulman怎么说](https://zhuanlan.zhihu.com/p/635151608)
