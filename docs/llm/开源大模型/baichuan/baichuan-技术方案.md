# Baichuan-7B 技术方案

Code: [Github](https://github.com/baichuan-inc/baichuan-7B)

Model: [Huggingface](https://huggingface.co/baichuan-inc/Baichuan-7B)

## 数据

中英文语料都包含. 通过打分和哈希, 对原始数据进行过滤, 提升数据质量. 对中英文数据比例, 各项任务的数据比例进行精调.

- 原始数据包括开源的中英文数据和自行抓取的中文互联网数据, 以及部分高质量知识性数据
- 频率和质量是数据处理环节重点考虑的两个维度
  - 基于启发式规则和质量模型打分, 对原始数据集进行篇章和句子粒度的过滤
  - 在全量数据上, 利用局部敏感哈希方法, 对篇章和句子粒度做滤重
- 经过不断的调整和多轮测试, 确认了一个在下游任务上表现最好的中英文配比
- 使用了一个基于自动学习的数据权重策略, 对不同类别的数据进行配比

## 分词

使用 SentencePiece 中的 byte pair encoding(BPE) 作为分词算法.

- 使用2000万条以中英为主的多语言语料训练 BPE 分词模型, 提升对于中文的压缩率
- 对数字的每一位单独分开, 避免出现数字不一致的问题, 对于提升数学能力有重要帮助
  - 方案同 LLaMA
- 对于罕见字词(如特殊符号等), 支持 UTF-8-characters 的 byte 编码, 因此做到未知字词的全覆盖
- 分词器对语料的压缩率为 0.737, 词表大小为 64000
  - 压缩率的单位是 `bytes/token`, 即平均每个 token 对应的字节数, 数字越小, 说明压缩的越厉害
  - LLaMA 对应的压缩率为 1.312, 相比 LLaMA 显著提高压缩效果, 词表大小为 32000
  - **高压缩率会带来训练和推理效率更高** 

## 模型结构

基于标准的 Transformer 结构, 采用了和 LLaMA 一样的模型架构设计.

- 位置编码: [RoPE](https://arxiv.org/abs/2104.09864)
- FFN 中的激活层: SwiGLU
- FFN 中间隐层大小为 11008, 输入大小为 4096, 倍数为 `8 / 3`
- Layer-Normalization: [RMSNorm](https://zhuanlan.zhihu.com/p/637675225)
- 使用 Pre-Normalization 结构
- 支持的 max context length 为 **4096**

## 训练方案

在原本的 LLaMA 框架上进行诸多修改以提升训练时的吞吐.

- 算子优化
  - Flash-attention, NVIDIA apex 的 RMSNorm 等
- 算子切分, 将部分计算算子进行切分, 减小内存峰值
- 混合精度技术, 降低在不损失模型精度的情况下加速计算过程
- 训练容灾, 训练平台和训练框架联合优化, IaaS + PaaS 实现分钟级的故障定位和任务恢复
- 通信优化
  - 根据卡数自适应设置 bucket size, 提高带宽利用率
  - 采用拓扑感知的集合通信算法, 避免网络拥塞问题, 提高通信效率
  - 根据模型和集群环境, 调优通信原语的触发时机, 从而将计算和通信重叠
