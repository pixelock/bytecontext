# Baichuan2-7B 技术方案

Paper: [Baichuan 2: Open Large-scale Language Models](https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf)

Code: [Github](https://github.com/baichuan-inc/Baichuan2)

## 预训练

### 数据集

#### 来源

从多个来源进行数据收集, 包括但不限于网页, 书籍, 研究论文, 代码等, 以构建一个广泛的世界知识系统.

#### 清洗

关注数据的频率和质量.

**数据的频率**依赖于聚类和去重. 构建了一个支持 LSH-like 特征和密集嵌入特征的大规模去重和聚类系统. 但这里有个问题是需要卡阈值, 去重过猛会影响多样性降低泛化能力

百川选择的做法是去除一部分, 并对剩余的样本打分, 作为预训练时采样的权重.

在数据处理的不同阶段, 训练数据的大小如下图所示. 绝对匹配去重 29.89% 数据, 启发式方法去除 1.77%, 句子级别的质量过滤 3%, 句子级别和段落级别去重 14.47%, 文档级别去重 19.13%.

![](/resources/images/llm/baichuan2-1.png)

**数据的质量**

- 采用句子级别的分类器进行过滤, 过滤掉低质量的句子.
- 对于内容安全, 用规则和模型洗掉有害内容. 还额外找了一些正向价值观的数据源, 提升采样概率.

### Tokenizer

分词器需要平衡两个关键因素: **高压缩率**以实现高效的推断, 以及**适当大小的词汇表**以确保每个词嵌入的充分训练.

Baichuan 2 使用的 tokenizer 是 BPE. 为了平衡计算效率和模型性能, 词汇表大小从Baichuan 1的 64000 扩展到 125696.

还有以下细节:

- 训练 BPE 分词器时, 不对输入文本应用任何规范化
- 训练参数: 字符覆盖率设置为0.9999, 稀有字符回退到 UTF-8 字节, 实现全覆盖
- 将数字分割成单个数字以更好地编码数值数据, 同 Baichuan 1
- 为了处理包含额外空格的代码数据, 专门增加空格 token

### 模型结构


