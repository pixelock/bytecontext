# Baichuan2-7B 技术方案

Paper: [Baichuan 2: Open Large-scale Language Models](https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf)

Code: [Github](https://github.com/baichuan-inc/Baichuan2)

## 预训练

### 数据集

#### 来源

从多个来源进行数据收集, 包括但不限于网页, 书籍, 研究论文, 代码等, 以构建一个广泛的世界知识系统.

#### 清洗

关注数据的频率和质量.

**数据的频率**依赖于聚类和去重. 构建了一个支持 LSH-like 特征和密集嵌入特征的大规模去重和聚类系统. 但这里有个问题是需要卡阈值, 去重过猛会影响多样性降低泛化能力

百川选择的做法是去除一部分, 并对剩余的样本打分, 作为预训练时采样的权重.

在数据处理的不同阶段, 训练数据的大小如下图所示. 绝对匹配去重 29.89% 数据, 启发式方法去除 1.77%, 句子级别的质量过滤 3%, 句子级别和段落级别去重 14.47%, 文档级别去重 19.13%.

![](/resources/images/llm/baichuan2-1.png)

**数据的质量**

- 采用句子级别的分类器进行过滤, 过滤掉低质量的句子.
- 对于内容安全, 用规则和模型洗掉有害内容. 还额外找了一些正向价值观的数据源, 提升采样概率.

### Tokenizer

分词器需要平衡两个关键因素: **高压缩率**以实现高效的推断, 以及**适当大小的词汇表**以确保每个词嵌入的充分训练.

Baichuan 2 使用的 tokenizer 是 BPE. 为了平衡计算效率和模型性能, 词汇表大小从Baichuan 1的 64000 扩展到 125696.

还有以下细节:

- 训练 BPE 分词器时, 不对输入文本应用任何规范化
- 训练参数: 字符覆盖率设置为0.9999, 稀有字符回退到 UTF-8 字节, 实现全覆盖
- 将数字分割成单个数字以更好地编码数值数据, 同 Baichuan 1
- 为了处理包含额外空格的代码数据, 专门增加空格 token

### 模型结构

#### 位置编码

7B的位置编码采用RoPE, 13B位置编码采用ALiBi. 主要是因为两种位置编码对模型效果基本没影响, 所以继承了Baichuan1的7B和13B的位置编码.

#### 激活函数

采用SwiGLU激活函数, 不同于传统FFN的2个矩阵, SwiGLU有三个矩阵, 因此缩小了隐藏层维度, 由原来的4倍变成8/3倍, 再调整为128的整数.

#### Normalization

采用了 RMSNorm 的实现, 提升计算效率. 对 Transformer 的输入进行采用 RMSNorm.

#### 提升训练稳定性

**NormHead**

对模型最终的输出的每个 token 的 embedding 表示进行归一化. 这是为了解决:

1. 低频 token 的模会在训练中变小, 干扰了训练的动态, 进行归一化后可以提升稳定性
2. 分析发现输出 token 的 embedding 表示, 语义关系受余弦相似度计算明显, 对L2距离不明显. 归一化可以减少线性分类器通过点积计算 logits 时, L2距离的影响. 从实验结果可以明显发现loss收敛更好更稳定

**Max-z loss**

在预训练时, logits 可能会变的非常大, **会导致推理过程中对惩罚因子不敏感**. 受 NormSoftmax 启发, 对 logits 进行归约, 作为损失的一部分, 主要有助于稳定训练并使推理对超参数更具鲁棒性.

$$
L_{\text{max-z}} = 2e^{-4} * z^{2}
$$

## 对齐阶段

### SFT

在监督微调阶段, 我们使用人类标注者为从各种数据源收集的提示进行标注. 每个提示都根据与 Claude 类似的关键原则被标记为有帮助或无害.

为了验证数据质量, 我们使用交叉验证: 权威标注者检查特定众包工作组注释的样本批次的质量, 过滤底质量数据.

SFT阶段总共使用了10万条SFT数据.

### Reward Model

为所有提示设计了一个三层分类系统, 包括6个主要类别, 30个次要类别和超过200个三级类别.

从用户的角度看, 希望分类系统全面覆盖所有类型的用户需求. 从奖励模型训练的角度看, 每个类别中的提示应该有足够的多样性, 以确保奖励模型能够很好地泛化.

RM数据使用百川 2 个不同大小和阶段的模型生成响应以增强响应多样性. RM 训练中只使用了百川 2 模型家族生成的response, 来自其他开源数据集和专有模型的response并没有提高奖励模型的准确性.

这里说明了偏好数据必须来自SFT或者PPO模型自身生成的, 保证了RM模型训练来自PPO同分布的数据, 说明了同分布数据对模型准确率的重要性.

### PPO

获得奖励模型后，我们使用 PPO 算法训练我们的语言模型。我们使用四个模型

- actor模型, 负责生成响应
- reference模型, 用于计算具有固定参数的KL惩罚
- reward模型, 为整个响应提供总体奖励
- critic模型, 设计为学习每个 token 的值

在RLHF训练过程中, critic模型首先提前进行了20个训练步骤的预热. 随后, 通过标准的PPO算法更新critic和actor模型. 对于所有模型, 我们使用0.5的梯度裁剪, 5e-6的恒定学习率和PPO裁剪阈值 `ϵ = 0.1`. 我们为所有的聊天模型设置了350次迭代，从而得到了Baichuan 2-7B-Chat 和 Baichuan 2-13B-Chat.

# 参考

- [[论文笔记]Baichuan 2: Open Large-scale Language Models](https://zhuanlan.zhihu.com/p/655606405)
- [百川的大模型KnowHow](https://zhuanlan.zhihu.com/p/655984589)
