# 从 GLM 到 ChatGLM

需要解决的任务, 以自然语言的指令, 即 Prompt 写出. 而语言模型很多时候对 Prompt 都非常敏感. Chat 类模型主要是为了降低 prompt 的难度诞生, 但 prompt 本身仍然可以有较大的影响. 询问同一个模型, 使用不同的问法, 往往会得到不同的答案.

GPT, GLM 这种**基座语言模型**, 输入一个问题, 续写生成后面文本, 一般来说基座模型的行为是对这个问题进行续写, 或者补充一些信息. 但我们期待的是对问题进行回答. 下图中我们对模型提问 `Explain the moon landing to a 6 year old in a few sentences.`, 我们期待地是对问题进行回答, 但 GLM(下左) 和 GPT-3(下右) 的回答是对问题进行了改写续写.

![](/resources/images/llm/glm-7.png)

训练 Chat 类模型的秘诀, 是通过 RLHF 训练, 让模型能更好的理解人类的意图, 并且将输入更好地与人类偏好对齐.

产生这种现象的原因是:

基座模型是在大规模**互联网语料**上进行训练的. 而互联网语料, 它并不是高质量分布的语料, 并且噪音很大, 很多内容并非符合人类偏好(e.g. 广告), 也不是回答问题的形式. 通过人类反馈, 引入人类的知识, 将模型的输出分布, 调整到人类偏好的高质量数据分布上.

OpenAI 在 2020 年发布 [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325) 中, 在文本摘要的任务上, 通过收集人类的反馈数据(偏好排序, 收集了 6.4k 对), 训练了一个奖励模型用户模拟人类反馈, 然后使用训练得到的奖励模型提供反馈, 通过强化学习来微调语言模型, 提升模型生成质量.

通过奖励模型, 有限地避免了收集人类的反馈是有限且昂贵的问题.
