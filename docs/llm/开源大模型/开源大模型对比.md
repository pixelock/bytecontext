| 模型表格 | 参数量 | 隐藏层维度 | 层数 | 注意力头数 | 训练数据(tokens) | 位置编码 | 激活函数 | 归一化方法 | 注意力机制 | 词表大小 | 最大长度 |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| LLAMA | 6.7B | 4096 | 32 | 32 | 1T | RoPE | SwiGLU | RMSNorm(pre-norm, Attention Layer和MLP的输入上使用) | 多头注意力机制(MHA) | 32000 | 2048 |
| LLAMA2 | 7B | 4096 | 32 | 32 | 2T | RoPE | SwiGLU | RMSNorm(pre-norm, Attention Layer和MLP的输入上使用) | GQA(Grouped-query attention) | 32000 | 4096 |
| Chatglm-6B | 6.2B | 4096 | 28 | 32 | 1T | RoPE 二维位置编码 | GELU | layer norm(deepnorm) | 多头注意力机制(MHA) | 130528 | 2048 |
| Chatglm2-6B | 6.2B | 4096 | 28 | 32 | 1.4T | RoPE | SwiGLU | layer norm(deepnorm) | Multi-Query Attention(MQA) | 65024 | 32768 |
| Baichuan-7b | 7B | 4096 | 32 | 32 | 1.2T | RoPE | SwiGLU | RMSNorm(pre-norm) | 多头注意力机制(MHA) | 64000 | 4096 |
| Baichuan2-7b | 7B | 4096 | 32 | 32 | 2.6T | RoPE | SwiGLU | RMSNorm(pre-norm) | 多头注意力机制(MHA), FlashAttention 实现 | 125696 | 4096 |
