# 数据并行

数据并行(Data Parallelism)的核心思想是, 在各个GPU上都拷贝一份完整模型, 各自使用不同的 mini-batch 数据, 算出各自的梯度, 最后对梯度进行累加来更新整体模型.

可以看到, 每个GPU都要存储一份完整的模型, 整体占用的GPU量是很大的. 另外, 每个GPU使用不同的数据计算出一批梯度, 在最终更新梯度之前, 所有GPU之间要相互同步各自的梯度, 汇总后才能进行梯度下降, 涉及到大量的GPU之间的通信. 因此, 数据并行系统的设计要考虑的重点为:

- 巨大的存储如何优化
- GPU间的通讯量如何最小化

# 数据并行的实现

目前有**DP**, **DDP**, **ZeRO**等几种常见的实现方式.

## DP(Data Parallelism)

最早的数据并行模式, 一般采用参数服务器(Parameters Server)框架, 多用于单机多卡.

### 整体架构

以下图说明 DP 的过程:

1. 有3块GPU, GPU0 ~ GPU2, 这三块GPU都是**计算GPU**. 另外有一块GPU作为**梯度收集GPU**, 进行**AllReduce**操作.
2. GPU0 ~ GPU2每一块计算GPU都拷贝一份**完整的**GPU3的模型参数.
3. 对于一个batch, 根据计算GPU的数量$$N$$(这里为3), 划分为 $$N$$ 个 mini-batch, 分别分配给GPU0 ~ GPU2.
4. 每块计算GPU使用各自的 mini-batch, 进行一轮前向传播和反向传播, 每个GPU都会计算得到一份梯度.
5. 每块计算GPU将自己计算得到的梯度推送(push)给梯度收集GPU, 在这块GPU上做**聚合操作**, 这里的聚合一般是梯度累加.
6. 梯度收集GPU聚合完成后, 每块计算GPU从它这里获取(pull)完成的梯度结果, 用户对各自的模型参数进行更新. 理论上各计算GPU上的模型参数始终保持一致.

其中梯度的聚合+下发的操作称为**AllReduce**

![](/resources/images/llm/dp-1.png)

DP 实现的是**参数服务器(Parameters Server)**框架. 在这个框架里, **计算GPU**称为**Worker**, **梯度聚合GPU**称为**Server**. 在实际应用中, 为了尽量减少通讯量, **一般可选择一个Worker同时作为Server**, 常常把GPU0同时作为Master和Worker.

另外这种框架通过配置还可以做到:

- 1个Worker或者Server下可以不止1块GPU
- Server可以只做梯度聚合, 也可以梯度聚合+全量参数更新一起做. 前者下发的是梯度, 后者下发的是模型参数. DP的过程又可以被描述下图:

![](/resources/images/llm/dp-2.png)

### 瓶颈

DP 有两个主要问题:

- **存储开销大**: 每块GPU上都存了一份完整的模型, 造成冗余
- **通讯开销大**: Server需要和每一个Worker进行梯度传输, 且整体的传输量与Worker的数量成正比. 当Server和Worker不在一台机器上时, Server的带宽将会成为整个系统的计算效率瓶颈. Worker的数量越多, 瓶颈问题约严重

### 问题优化

当通信带宽称为瓶颈时, Server在获取数据的同时, Worker都在空闲. 为了提高系统的效率, 提出了**梯度异步更新**.

下图为梯度异步更新方案下, 某一个Worker的行为:

1. 在执行第10轮的计算时, 该Worker会正常计算梯度, 并向Server发送push和pull梯度的请求.
2. 但该Worker**并不会实际等到把聚合梯度拿回来**, 而是直接拿Worker上旧的未更新的模型参数, 直接使用新的数据, 开始第11轮的计算. 这样Worker在通讯的时间里, 并行地做新一轮的计算, 从而提升了计算通讯比.

![](/resources/images/llm/dp-3.png)

这种异步方式, 存在只计算梯度, 不更新权重的情况, 会造成一定程度的Workers之间参数不同步的问题. 如果通信延迟严重, 就会导致模型**无法收敛**. 在使用过程中, 可以选择三种模式:

- 无延迟. 各Worker完全同步, 但计算通信比低, 训练效率低
- 延迟但不指定延迟步数. 完全不加任何限制的, 可能存在Workers之间的权重新老差别严重的问题, 产生的梯度无法使模型收敛
- 延迟且指定延迟步数为1. 结合下图, 在做迭代3时, 迭代2的结果可以不用等待, 但必须保证迭代0, 1的梯度都已经被Server处理完毕, 并可以pull下来进行模型参数更新. 这样限制了不同Workers之间模型参数差别过大的问题, 提升训练效率的同时, 也缓解了收敛问题

![](/resources/images/llm/dp-4.png)

## DDP(Distributed Data Parallelism)

分布式数据并行, 采用 **Ring AllReduce** 的通讯方式. 可以用于单机多卡, 也可以用于多机多卡. DDP解决的就是通讯问题, **将Server上的通讯压力均衡转到各个Worker上**, 去除Server, 保留Worker.

实现这一点是通过优化AllReduce过程来实现的. DDP使用**Ring-AllReduce**.

如下图, 假设有4块GPU, 每块GPU上的数据也对应被切成4份, AllReduce的最终目标, 就是让每块GPU上的数据都变成箭头右边汇总的样子. 实现为定义网络拓扑关系, 使得每个GPU只和其相邻的两块GPU通讯, 充分利用设备和设备之间的带宽.

![](/resources/images/llm/ring-allreduce-3.png)

**Ring-AllReduce操作的对象为每个GPU设备计算得到的梯度. 让每个GPU中每个位置最终的梯度都聚合了所有GPU对应位置的梯度, 在进行更新, 从而保证了所有GPU上梯度的同步, 进而保证了模型参数的同步.**

Ring-AllReduce则分两大步骤实现该目标: **Reduce-Scatter**和**All-Gather**.

**Reduce-Scatter**

![](/resources/images/llm/ring-allreduce-2.png)

从上图中可以看出, Reduce-Scatter的结果是**每个设备保存一部分 reduce 之后的结果**. 分步来看:

![](/resources/images/llm/reduce-scatter-1.jpg)

![](/resources/images/llm/reduce-scatter-2.jpg)

在第1步中, 每个设备都负责某一数据块, 并向右边的设备发送这块数据. 例如 GPU0 负责将第1片数据发送给 GPU1. 每个 GPU 设备在向右推送的同时接受左边设备的数据(上下行并行, 双工通信, 带宽相同), 在一次reduce完成后, 蓝色数据块被更新, **被更新的数据块作为该设备下一次发起推送的起点**, 得到以下的结果:

![](/resources/images/llm/reduce-scatter-3.jpg)

这样每个GPU设备第一次reduce的结果都被推送给右边的设备, 并进行第二次reduce, 因此绿色数据块被更新, 收集到了这两次reduce的结果. 并作为下一轮推送的起点.

![](/resources/images/llm/reduce-scatter-4.jpg)

再进行一次更新后, 红色数据块就聚集了所有 GPU 设备对应位置的初始数据. Reduce-Scatter阶段结束. 完整的过程如下图(换成了向左传递, 通过逐渐变深的颜色表示数据累加的次数更多):

![](/resources/images/llm/reduce-scatter-1.png)

**All-Gather**

![](/resources/images/llm/all-gather-2.png)

All-Gather也是以通过环状通信算法来实现. 与Reduce-Scatter不同的是reduce操作由相加变成了**替代**. 以Reduce-Scatter结束时得到的红色数据块为起点, 将每个 GPU 设备中红色数据块的结果传播到所有 GPU 设备上. 完整过程如下:

![](/resources/images/llm/all-gather-1.png)

**All-Reduce**

通过**Reduce-Scatter**和**All-Gather**两步, **让每个设备上的矩阵里的每一个位置的数值都是所有设备上对应位置的数值之和**. 再次贴出这张图:

![](/resources/images/llm/ring-allreduce-3.png)

### Ring-AllReduce通讯量分析

假设模型参数W的大小为 $$\Phi$$, GPU 数量为 $$N$$, 每个梯度块的大小为$$\frac{\Phi}{N}$$. 现在我们考虑单卡的通讯量. 由于是双工通信, 即每个设备出口和入口带宽可以同时达到带宽限制, 所以send的通讯量为例.

- Reduce-Scatter

---

# 参考资料

- [图解大模型训练之：数据并行上篇(DP, DDP与ZeRO)](https://zhuanlan.zhihu.com/p/617133971)
- [DeepSpeed之ZeRO系列：将显存优化进行到底](https://zhuanlan.zhihu.com/p/513571706)
- [手把手推导Ring All-reduce的数学性质](https://zhuanlan.zhihu.com/p/504957661)
