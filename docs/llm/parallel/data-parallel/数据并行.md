# 数据并行

数据并行(Data Parallelism)的核心思想是, 在各个GPU上都拷贝一份完整模型, 各自使用不同的 mini-batch 数据, 算出各自的梯度, 最后对梯度进行累加来更新整体模型.

可以看到, 每个GPU都要存储一份完整的模型, 整体占用的GPU量是很大的. 另外, 每个GPU使用不同的数据计算出一批梯度, 在最终更新梯度之前, 所有GPU之间要相互同步各自的梯度, 汇总后才能进行梯度下降, 涉及到大量的GPU之间的通信. 因此, 数据并行系统的设计要考虑的重点为:

- 巨大的存储如何优化
- GPU间的通讯量如何最小化

# 数据并行的实现

目前有**DP**, **DDP**, **ZeRO**等几种常见的实现方式.

## DP(Data Parallelism)

最早的数据并行模式, 一般采用参数服务器(Parameters Server)框架, 多用于单机多卡.

### 整体架构

以下图说明 DP 的过程:

1. 有3块GPU, GPU0 ~ GPU2, 这三块GPU都是**计算GPU**. 另外有一块GPU作为**梯度收集GPU**, 进行**AllReduce**操作.
2. GPU0 ~ GPU2每一块计算GPU都拷贝一份**完整的**GPU3的模型参数.
3. 对于一个batch, 根据计算GPU的数量$$N$$(这里为3), 划分为 $$N$$ 个 mini-batch, 分别分配给GPU0 ~ GPU2.
4. 每块计算GPU使用各自的 mini-batch, 进行一轮前向传播和反向传播, 每个GPU都会计算得到一份梯度.
5. 每块计算GPU将自己计算得到的梯度推送(push)给梯度收集GPU, 在这块GPU上做**聚合操作**, 这里的聚合一般是梯度累加.
6. 梯度收集GPU聚合完成后, 每块计算GPU从它这里获取(pull)完成的梯度结果, 用户对各自的模型参数进行更新. 理论上各计算GPU上的模型参数始终保持一致.

其中梯度的聚合+下发的操作称为**AllReduce**

![](/resources/images/llm/dp-1.png)

DP 实现的是**参数服务器(Parameters Server)**框架. 在这个框架里, **计算GPU**称为**Worker**, **梯度聚合GPU**称为**Server**. 在实际应用中, 为了尽量减少通讯量, **一般可选择一个Worker同时作为Server**, 常常把GPU0同时作为Master和Worker.

另外这种框架通过配置还可以做到:

- 1个Worker或者Server下可以不止1块GPU
- Server可以只做梯度聚合, 也可以梯度聚合+全量参数更新一起做. 前者下发的是梯度, 后者下发的是模型参数. DP的过程又可以被描述下图:

![](/resources/images/llm/dp-2.png)

### 瓶颈

DP 有两个主要问题:

- **存储开销大**: 每块GPU上都存了一份完整的模型, 造成冗余
- **通讯开销大**: Server需要和每一个Worker进行梯度传输, 且整体的传输量与Worker的数量成正比. 当Server和Worker不在一台机器上时, Server的带宽将会成为整个系统的计算效率瓶颈. Worker的数量越多, 瓶颈问题约严重

### 问题优化

当通信带宽称为瓶颈时, Server在获取数据的同时, Worker都在空闲. 为了提高系统的效率, 提出了**梯度异步更新**.

下图为梯度异步更新方案下, 某一个Worker的行为:

1. 在执行第10轮的计算时, 该Worker会正常计算梯度, 并向Server发送push和pull梯度的请求.
2. 但该Worker**并不会实际等到把聚合梯度拿回来**, 而是直接拿Worker上旧的未更新的模型参数, 直接使用新的数据, 开始第11轮的计算. 这样Worker在通讯的时间里, 并行地做新一轮的计算, 从而提升了计算通讯比.

![](/resources/images/llm/dp-3.png)

这种异步方式, 存在只计算梯度, 不更新权重的情况, 会造成一定程度的Workers之间参数不同步的问题. 如果通信延迟严重, 就会导致模型**无法收敛**. 在使用过程中, 可以选择三种模式:

- 无延迟. 各Worker完全同步, 但计算通信比低, 训练效率低
- 延迟但不指定延迟步数. 完全不加任何限制的, 可能存在Workers之间的权重新老差别严重的问题, 产生的梯度无法使模型收敛
- 延迟且指定延迟步数为1. 结合下图, 在做迭代3时, 迭代2的结果可以不用等待, 但必须保证迭代0, 1的梯度都已经被Server处理完毕, 并可以pull下来进行模型参数更新. 这样限制了不同Workers之间模型参数差别过大的问题, 提升训练效率的同时, 也缓解了收敛问题

![](/resources/images/llm/dp-4.png)

## DDP(Distributed Data Parallelism)

分布式数据并行, 采用 **Ring AllReduce** 的通讯方式. 可以用于单机多卡, 也可以用于多机多卡. DDP解决的就是通讯问题, **将Server上的通讯压力均衡转到各个Worker上**, 去除Server, 保留Worker.

实现这一点是通过优化AllReduce过程来实现的. DDP使用**Ring-AllReduce**.

如下图, 假设有4块GPU, 每块GPU上的数据也对应被切成4份, AllReduce的最终目标, 就是让每块GPU上的数据都变成箭头右边汇总的样子. 实现为定义网络拓扑关系, 使得每个GPU只和其相邻的两块GPU通讯, 充分利用设备和设备之间的带宽.

![](/resources/images/llm/ring-allreduce-3.png)

**Ring-AllReduce操作的对象为每个GPU设备计算得到的梯度. 让每个GPU中每个位置最终的梯度都聚合了所有GPU对应位置的梯度, 在进行更新, 从而保证了所有GPU上梯度的同步, 进而保证了模型参数的同步.**

Ring-AllReduce则分两大步骤实现该目标: **Reduce-Scatter**和**All-Gather**.

### Reduce-Scatter

![](/resources/images/llm/ring-allreduce-2.png)

从上图中可以看出, Reduce-Scatter的结果是**每个设备保存一部分 reduce 之后的结果**. 分步来看:

![](/resources/images/llm/reduce-scatter-1.jpg)

![](/resources/images/llm/reduce-scatter-2.jpg)

在第1步中, 每个设备都负责某一数据块, 并向右边的设备发送这块数据. 例如 GPU0 负责将第1片数据发送给 GPU1. 每个 GPU 设备在向右推送的同时接受左边设备的数据(上下行并行, 双工通信, 带宽相同), 在一次reduce完成后, 蓝色数据块被更新, **被更新的数据块作为该设备下一次发起推送的起点**, 得到以下的结果:

![](/resources/images/llm/reduce-scatter-3.jpg)

这样每个GPU设备第一次reduce的结果都被推送给右边的设备, 并进行第二次reduce, 因此绿色数据块被更新, 收集到了这两次reduce的结果. 并作为下一轮推送的起点.

![](/resources/images/llm/reduce-scatter-4.jpg)

再进行一次更新后, 红色数据块就聚集了所有 GPU 设备对应位置的初始数据. Reduce-Scatter阶段结束. 完整的过程如下图(换成了向左传递, 通过逐渐变深的颜色表示数据累加的次数更多):

![](/resources/images/llm/reduce-scatter-1.png)

### All-Gather

![](/resources/images/llm/all-gather-2.png)

All-Gather也是以通过环状通信算法来实现. 与Reduce-Scatter不同的是reduce操作由相加变成了**替代**. 以Reduce-Scatter结束时得到的红色数据块为起点, 将每个 GPU 设备中红色数据块的结果传播到所有 GPU 设备上. 完整过程如下:

![](/resources/images/llm/all-gather-1.png)

### All-Reduce

通过**Reduce-Scatter**和**All-Gather**两步, **让每个设备上的矩阵里的每一个位置的数值都是所有设备上对应位置的数值之和**. 再次贴出这张图:

![](/resources/images/llm/ring-allreduce-3.png)

### Ring-AllReduce通讯量分析

假设模型参数W的大小为 $$\Phi$$, GPU 数量为 $$N$$, 每个梯度块的大小为$$\frac{\Phi}{N}$$. 现在我们考虑单卡的通讯量. 由于是双工通信, 即每个设备出口和入口带宽可以同时达到带宽限制, 所以send的通讯量为例.

- Reduce-Scatter 阶段, 单卡对外发送的通讯量为 $$(N - 1) \frac{\Phi}{N}$$
- All-Gather 阶段, 单卡对外发送的通讯量为 $$(N - 1) \frac{\Phi}{N}$$

单卡的总通讯量为 $$2(N - 1) \frac{\Phi}{N}$$, 随着卡数的增多, 单卡的通讯量逼近上限 $$2\Phi$$.

首先可以看到, 架构中所有 GPU 是平等的, 没有 SERVER 和 WORKER 之分. 而每张卡的通信量约为 $$2\Phi$$, 与架构中卡的数量无关. 在 DP 架构中, 虽然每张 WORKER 的通讯量为 $$\Phi$$, 但 SERVER 承载的通讯量为 $$N\Phi$$, 当系统架构中的 GPU 设备越多时, SERVER 的压力越大, 阻塞越严重. 而 DDP 这种架构很好的解决了这个问题.

---

## ZeRO

DDP 架构很好地解决了 GPU 之间**通讯负载不均衡**的问题. 但还存在**显存开销大**的问题. 每个 GPU 上都复制了一份完整的模型参数, 存在冗余的现象. 而[ZeRO(零冗余优化)](https://arxiv.org/abs/1910.02054)的出现, 就是为了解决这个问题.

### 显存都在存储什么

在[Transformer相关数值](/docs/nlp/models/transformers/Transformer相关数值.md)一节中, 我们分析了在**训练**过程中, 显存被什么占用. 这里在简单回顾下.

![](/resources/images/llm/zero-1.png)

在使用[混合精度训练](/docs/llm/quantization/混合精度训练.md)和Adam优化器的作为训练标配的背景下, 显存存储主要分为两大块:

**Model States 模型状态**

模型状态包括**计算过程中使用的模型参数**(fp16, $$2\Phi$$), **模型梯度**(fp16, $$2\Phi$$), Adam优化器中的**模型参数备份**(fp32, $$4\Phi$$), **momentum**(fp32, $$4\Phi$$) 和 **variance**(fp32, $$4\Phi$$).

共计: 模型参数$$2\Phi$$ + 模型梯度$$2\Phi$$ + Adam状态$$12\Phi$$ = $$16\Phi$$.

**Residual States 剩余状态**

在训练过程中会额外产生的内容, 包括:

- activation, 激活值. 在backward过程中使用链式法则计算梯度时会用到, 有了它算梯度会更快, 但它不是必须存储的, 因为可以通过重新做Forward来算它
- temporary buffers, 临时存储. 例如把梯度发送到某块GPU上做加总聚合时产生的存储
- unusable fragment memory, 碎片化的存储空间. 虽然总存储空间是够的, 但是如果取不到连续的存储空间, 申请显存的请求也会被Fail掉. 对这类空间浪费可以通过内存整理来解决

### ZeRO 原理

针对模型状态去除冗余的存储优化, ZeRO使用的方法是分片(partition). 即每张卡只存 $$\frac{1}{N}$$ 的模型状态量, 这样系统内只维护一份模型状态.

![](/resources/images/llm/zero-2.png)

#### ZeRO-1

首先从 **Adam optimizer state** 进行分片操作. 也就是上图中的 $$P_{\text{os}}$$. 此时, optimizer state 在每张卡的占用为 $$\frac{12\Phi}{N}$$, 而模型参数和梯度仍旧是每张卡保持一份, 所以每张卡的模型状态所需显存是 $$4\Phi + \frac{12\Phi}{N}$$. 当 $$N$$ 比较大时, 每张卡的显存占用趋向于 $$4\Phi$$, 即原来的 `1/4`.

#### ZeRO-2

继续对**模型梯度**(gradients)进行分片, 也就是上图中的$$P_{\text{os+g}}$$. 模型参数仍旧是每张卡保持一份, 此时每张卡的模型状态所需显存是$$2\Phi + \frac{14\Phi}{N}$$. 当 $$N$$ 比较大时, 每张卡的显存占用趋向于 $$2\Phi$$, 即原来的 `1/8`.

#### ZeRO-3

最后对模型参数(parameters)进行分片, 也就是上图中的$$P_{\text{os+g+p}}$$. 此时每张卡的模型状态所需显存是$$\frac{16\Phi}{N}$$. 当 $$N$$ 比较大时, 每张卡的显存占用趋向于0.

### 通讯量分析

每张卡的显存占用降了下来, 但分片的操作是否带来了额外的通讯开销? 如果通信开销过大的话, 会拖累训练速度.

首先说结论: $$P_{\text{os}}$$, $$P_{\text{os+g}}$$**通信量和传统数据并行相同**, $$P_{\text{os+g+p}}$$**会增加通信量**.

传统数据数据并行在每一步计算梯度后, 需要进行一次**Ring AllReduce**操作来计算梯度均值, 这一步分为ReduceScatter和AllGather两小步, 总计每张卡的通信数据量为$$2\Phi$$.

直接分析 $$P_{\text{os+g}}$$, 每张卡只存储 $$\frac{1}{N}$$ 的优化器状态和梯度, 对于 GPU0 来说, 为了计算它这 $$\frac{1}{N}$$ 梯度的均值, 需要进行一次 **Reduce** 操作, 其余每张卡只发送一次, 通信量为 $$\frac{\Phi}{N} * N = \Phi$$, 其余显卡则不需要保存这部分梯度值了.

当 GPU0 计算好梯度均值后, 就可以更新局部的优化器状态. 当反向传播过程结束, 进行一次 **Gather** 操作, 收集到剩余 $$(1 - \frac{1}{N})\Phi$$ 部分的模型参数, 通讯量为 $$\frac{\Phi}{N} * N = \Phi$$.

上面分析的单张卡的通讯过程. 从全局来看, 相当于完成 **Reduce-Scatter** 和 **AllGather** 两步, 和传统数据并行进行 **Ring AllReduce** 的过程是一样的, 通讯量也是相同的, 都为$$2\Phi$$.

而对于 $$P_{\text{os+g+p}}$$, 每张卡都只存了 $$\frac{1}{N}$$ 的参数, 不管是在前向计算还是反向传播, 都涉及一次 **All-Gather** 操作, 取回其他 GPU 上的模型参数. 两次 All-Gather 操作的通讯量为$$2\Phi$$. 在 BWD 之后, 每块 GPU 算出的完整梯度需发送给对应分片的 GPU, 整体上来看是执行了一次 **Reduce-Scatter**, 对应的通讯量为 $$\Phi$$. 总体的通讯量为$$3\Phi$$.

即使用**ZeRO-1**和**ZeRO-2**的通讯量完全没有增加, 而**ZeRO-3**的通讯量是常规DP的1.5倍(下图的 `Comm Volume` 一列).

![](/resources/images/llm/zero-3.png)

### ZeRO = DP + MP?

ZeRO对数据并行的优化(解决显存冗余开销大的问题)的方法, 是通过**模型并行**来实现的. 因此 ZeRO 实际上是 **数据并行(DP) + 模型并行(MP)的形式**, 但**本质上仍是在做数据并行(DP)的事情**.

因为模型并行(无论是PP或TP), 

---

# 参考资料

- [图解大模型训练之：数据并行上篇(DP, DDP与ZeRO)](https://zhuanlan.zhihu.com/p/617133971)
- [图解大模型训练之：数据并行下篇(DeepSpeed ZeRO，零冗余优化)](https://zhuanlan.zhihu.com/p/618865052)
- [DeepSpeed之ZeRO系列：将显存优化进行到底](https://zhuanlan.zhihu.com/p/513571706)
- [手把手推导Ring All-reduce的数学性质](https://zhuanlan.zhihu.com/p/504957661)
