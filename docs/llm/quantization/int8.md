从FP32到FP16/BF16, 模型占用的内存大小减半, 如果像进一步缩小内存的占用, 还是从浮点数的角度出发, 如使用FP8, 留下的位数太少, 能表示的数值范围和精度都非常有限, 实际肯定是不可用的. 训练或推理的精度会急剧下降.

为了解决这个问题, 我们引入了 8 位量化, 在不损失多少推理精度的同时, 将模型大小压缩到FP32的`1/4`.

# 什么是 INT8

8 位量化, 不是简单地丢弃一半位宽, 关键在于量化. 量化过程是**从一种数据类型舍入到另一种数据类型**. 例如, 如果一种数据类型的范围为$$0,\cdots,9$$, 而另一种数据类型的范围为$$0,\cdots,4$$, 则第一种数据类型中的值 4 将舍入为第二种数据类型中的 2. 但是, 如果在第一种数据类型中有值 3, 它介于第二种数据类型的 1 和 2 之间, 那么我们通常会四舍五入为 2.

这表明量化是一个有噪过程, 会导致信息丢失, 是一种有损压缩.

两种最常见的 8 位量化技术是:

- **零点量化(zero-point quantization)**
- **最大绝对值量化(absolute maximum quantization, absmax)**

它们都将浮点值映射为更紧凑的单字节 Int8 值. 这些方法的**第一步都是用量化常数对输入进行归一化缩放**.

## 零点量化(zero-point quantization)

![](/resources/images/llm/float-4.png)

简而言之, 零点量化分为两步:

- 第一步**值域映射**: 即通过缩放将原始的数值范围映射为量化后的数值范围
- 第二步**零点调整**: 即通过平移将映射后的数据的最小值对齐为目标值域的最小值

如果我的数值范围是$$[-1.0, 1.0]$$, 想量化到$$[-127, 127]$$, 需要先缩放 127 倍, 后四舍五入到 8 位精度. 反过来, 要恢复原始值, 需要将 Int8 值除以相同的量化因子 127.

例如值 0.3 将缩放为 $$0.3 * 127 = 38.1$$, 四舍五入后得到值 38. 恢复时, 我们会得到 $$38 / 127 = 0.2992$$, 因此最终会有 0.008 的量化误差. **这些看似微小的误差在沿着模型各层传播时往往会累积和增长, 从而导致最终的精度下降**.

## 最大绝对值量化(absmax)

要计算 absmax 量化中 fp16 数与其对应的 int8 数之间的映射, 必须**先除以张量的最大绝对值, 然后再乘以数据类型的最大可表示值**. 例如, 对向量 `[1.2, -0.5, -4.3, 1.2, -3.1, 0.8, 2.4, 5.4]` 进行量化, 首先需要计算该向量元素的最大绝对值, 本例中为 4.5. Int8 的范围为 `[-127, 127]`, 因此我们将 127 除以 5.4，得到缩放因子 23.5, 最后, 将原始向量乘以缩放因子得到最终的量化向量 `[28, -12, -101, 28, -73, 19, 56, 127]`.

要恢复原向量, 可以将 int8 量化值除以缩放因子, 但由于上面的过程是四舍五入的, 将丢失一些精度.

![](/resources/images/llm/float-6.png)

# 参考资料

- [大规模 Transformer 模型 8 比特矩阵乘简介 - 基于 Hugging Face Transformers、Accelerate 以及 bitsandbytes](https://huggingface.co/blog/zh/hf-bitsandbytes-integration)
