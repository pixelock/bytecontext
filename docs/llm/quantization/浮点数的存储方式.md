# 什么是浮点数

计算机中的小数存储, 根据**小数点是否固定**, 分为**定点数**和**浮点数**. 注意, 定点数也可以表示小数,  浮点数同样可以表示小数和整数, 定点数和浮点数只是计算机表示数据的两种不同方式而已.

IEEE标准中, 浮点数由符号位(S), 指数(EXP), 尾数(FRAC)三部分组成.

如`178.125`转化为二进制为`10110010.001`, 又可表示为`1.0110010001`乘以2的7次方, 7用二进制`111`表示. 这里`10110010001`称为尾数, `111`称为指数, 另外还有一个0或1表示数值的正负.

浮点数根据总位数的不同, 分为**double(64bit)**, **float(32bit)**, **fp(16bit)**:

![](/resources/images/llm/float-1.png)

以`fp16`为例, 每个数使用16位/两个字节存储. 存储格式为1个符号位, 5个指数位, 10个尾数位.

![](/resources/images/llm/float-2.png)

与十进制的转换关系为:

$$
\text { data }= \begin{cases}(-1)^{\text {sign }} \times 2^{(\text {exp} - 15)} \times\left(1+\frac{\text { fraction }}{1024}\right), & \text { exp } \neq 0 \\ (-1)^{\text {sign }} \times 2^{-14} \times\left(0+\frac{\text { fraction }}{1024}\right), & \text { exp }=0\end{cases}
$$

`fp16`的数值范围如下, 对应的精度为$$2^{-24}$$.

$$
(-1)^0 \times 2^{-14} \times\left(0+\frac{1}{1024}\right) \approx 6 \times 10^{-8} \sim(-1)^0 \times 2^{15} \times\left(1+\frac{1023}{1024}\right) \approx 65504
$$

float, double与fp16相比, 拥有更大的数值范围, 以及更细的精度.

---

# 常见的浮点数类型

![](/resources/images/llm/float-3.png)

## Float32

Float32(FP32)是标准的 IEEE 32 位浮点表示. 在 FP32 中, 为指数保留了8位, 为尾数保留了23位, 为符号保留了1位. float32对应的数据范围为:

$$
1.4 \times 10^{-45} \sim 1.7 \times 10^{38}
$$

## FP16 和 BF16

**Float16(FP16)** 每个数使用16位/两个字节存储. 存储格式为1个符号位, 5个指数位, 10个尾数位. **这使得FP16数字的数值范围远低于FP32**, 因此 FP16存在上溢和下溢的风险. 这里:

- **上溢**: 当用于表示非常大的数时
- **下溢**: 当用于表示非常精细的小数时

例如数字`1,000,000`, FP16无法表示该数, 因为FP16能表示的最大数大约是64K, 在Pytorch等框架中, 最终会得到`NaN`(Not a Number). 一旦出现`NaN`, 之前的所有计算就全毁了.

一种新格式 **Bfloat16(BF16)** 可以来缓解这种情况. BF16为指数保留8位(与FP32相同), 尾数位为7位. 这意味着使用BF16我们可以保留与FP32相同的动态范围, 但是相对于FP16, 损失了3位精度. 因此, 在使用BF16精度时, 大数值绝对没有问题, 但是精度会比FP16差.

**总结**:

- FP16相比BF16, 拥有更好的精度, 但数值范围窄很多

---

# 参考资料

- [大规模 Transformer 模型 8 比特矩阵乘简介 - 基于 Hugging Face Transformers、Accelerate 以及 bitsandbytes](https://huggingface.co/blog/zh/hf-bitsandbytes-integration)
- [BF16 与 FP16 在模型上哪个精度更高呢](https://blog.csdn.net/u013250861/article/details/131152163)
