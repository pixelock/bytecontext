# 为什么需要混合精度

当前深度学习框架大都采用的都是 fp32 来进行权重参数的存储. 随着模型越来越大, 继续使用 fp32 存在问题:

- 模型参数量越来越大, 对显存容量的要求越来越高
- fp32 相较于 fp16 等低位数占用计算量大, 训练和推理时间都慢

## 混合精度训练(Mixed Precision Training)

混合精度训练指的是, 在训练过程中, 同时存在 fp32 和 fp16/bf16 类型的数值. 精度减半, 显存占用和计算时间都能减半, 训练的时候可以使用更大的 Batch size, 做到更大的数据吞吐量.

但使用半精度参与训练, 会存在两个问题:

- **舍入误差**: fp16 的最大舍入误差约为 `~2 ^-10` (参考[浮点数的存储方式](/docs/llm/quantization/浮点数的存储方式.md)), 比 fp32 的最大舍入误差 `~2 ^-23` 要大不少. 这会造成**对足够小的浮点数执行的任何操作都会将该值四舍五入到零**. 在反向传播中很多甚至大多数梯度更新值都非常小(但不为零), 舍入误差累积可以把这些数字变成 0 或者 NaN, 会导致不准确的梯度更新, 影响网络的收敛

- **溢出错误**: 由于 fp16 的有效的动态范围约为 `5.96×10^-8 ~ 6.55×10^4`, 比 fp32 的范围 `1.4x10^-45 ~ 1.7x10^38` 要狭窄很多, 会导致得到的值大于或者小于 fp16 的有效动态范围, 也就是**上溢出**或者**下溢出**

解决溢出错误的方法是通过**损失缩放**解决下溢出的问题. 训练中激活梯度的值太小, 造成了下溢出, 损失缩放是指在执行反向传播之前, 将损失函数的输出乘以某个标量数, 乘性增加的损失值产生乘性增加的梯度更新值, 提升许多梯度更新值到超过 fp16 的安全阈值. 确保在应用梯度更新之前撤消缩放, 并且不要选择一个太大的缩放以至于产生 `inf` 上溢出.

混合精度训练的思路是保证模型整体的精度的前提下, 尽可能地在 fp16 这种计算高效类型上多操作来大幅度减少神经网络训练时间. 因此一般在 `forward` 和 `backward` 这些计算量很大的步骤中使用 fp16, 在模型参数更新, 优化器状态值更新这些需要高精度**避免累加舍入误差**的 `Reduction` 运算中使用 fp32.

在使用Adam类型的优化器训练时, 混合精度训练一般采用如下的方式:

![](/resources/images/llm/mixed-1.png)

![](/resources/images/llm/mixed-2.png)

- 在计算过程中, 模型参数, 模型梯度, 中间的 `Activations` 使用 fp16, 加速计算
- 模型权重备份, 优化器的 `momentum`, `variance` 使用fp32, 避免累计舍入误差

## 混合精度训练的显存占用分析

下面我们来分析使用 fp32 和 fp16 进行混合精度训练时, 显存的占用情况.

假设模型的参数量为$$\Phi$$, 参考上图, 模型训练中的状态内存占用分为:

- 模型参数备份, Adam优化器中的 `momentum` 和 `variance`. 这三部分都使用 fp32 类型存储, 对应的字节占用大小为 $$4\Phi + 4\Phi + 4\Phi = 12\Phi$$
- 模型参数. 这部分使用 fp16, 对应字节大小为 $$2\Phi$$
- 模型梯度. 这部分使用 fp16, 对应字节大小为 $$2\Phi$$

总计$$16\Phi$$.

---

# 参考资料

- [DeepSpeed之ZeRO系列：将显存优化进行到底](https://zhuanlan.zhihu.com/p/513571706)
- [pytorch模型训练之fp16、apm、多GPU模型、梯度检查点（gradient checkpointing）显存优化等](https://zhuanlan.zhihu.com/p/448395808)
- [PyTorch的自动混合精度（AMP）](https://zhuanlan.zhihu.com/p/165152789)
