# 读取本地数据集

从本地文件中读取数据集, Datasets 支持以下几种常见的数据形式:

| Data format | Loading script | Example |
| --- | --- | --- |
| CSV & TSV | csv | `load_dataset("csv", data_files="my_file.csv")` |
| Text files | text | `load_dataset("text", data_files="my_file.txt")` |
| JSON & JSON Lines | json | `load_dataset("json", data_files="my_file.jsonl")` |
| Pickled DataFrames | pandas | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

## 读取 JSON 数据

以 [SQuAD-it dataset](https://github.com/crux82/squad-it/) 数据集为例, 这是一个大规模的意大利语QA(question answering)任务数据集.

首先, 使用以下的命令, 将训练数据集和测试数据集全部下载到本地. 由于是压缩文件, 还需要经过一步解压缩, 得到两个`json`文件:

```shell
wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
gzip -dkv SQuAD_it-*.json.gz
```

查看`json`文件中数据的形式:

```json
{
    "data": [
        {
            "title": "Terremoto del Sichuan del 2008",
            "paragraphs": [
                {
                    "context": "Il terremoto del Sichuan del 2008 o il terremoto del Gran Sichuan, misurato a 8.0 Ms e 7.9 Mw",
                    "qas": [
                        {
                            "id": "56cdca7862d2951400fa6826",
                            "answers": [
                                {
                                    "text": "69.197",
                                    "answer_start": 232
                                }
                            ],
                            "question": "Quante persone sono state uccise come risultato?"
                        },
                        ...
                    ]
                },
                ...
            ]
        },
        ...
    ]
}
```

简单描述数据集的格式.

- 所有的样本以列表的形式排列, 放在`data`这个字段下面
- 每个样本是一篇文章, 每篇文章包含`title`和`paragraphs`两个字段. 其中`title`字段表示文章的标题, `paragraphs`表示文章的内容, 并以段落的形式展开. 由于一篇文章内容有多个段落, 所以这个字段的值也是列表, 每个元素代表一段内容
- 一段内容中, 包含`context`和`qas`两个字段. `context`代表是这段的内容文本, `qas`代表的是与这段内容相关的**问题答案对**. 一段内容可能对应多个问答对, 因此`qas`字段的值也是一个列表, 每个元素代表一个问答对
- 具体到问答对, 每一对包含`id`, `question`, `answers`三个字段, 分别代表问答对的ID, 问题和答案. 其中的`answers`可能包含一个或多个正确答案, 因此`answers`字段下的值也是一个列表, 列表中每个元素代表一个正确答案
- 答案包含两个字段`text`和`answer_start`, 分别代表答案文本, 以及答案文本在段落中的起始位置

所有的样本都在 `data` 这个字段下面, 那么在读取数据集的时候, 我们需要通过 `field` 这个参数指定数据存在的字段:

```python
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
squad_it_dataset
```

使用 `load_dataset` 从本地读取数据集, 将会返回一个 **`DatasetDict`** 对象, 类似字典类型, `key` 为数据集的划分类型, 常见的有: `train`, `eval`, `test` 等, 值 `value` 是这个划分下的数据集, 是一个 `Dataset` 对象.

在从本地文件读取数据集时, 如果给 `data_files` 参数是**一个**文件(像上面的例子), 则默认为 `train` 划分, 返回结果如下:

```python
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

由于每个样本是一篇文章, 在 `json` 原文件中也可以看到, 文章对应着 `title`, `paragraphs` 两个字段. 因此返回的数据集中也是有这两列, 列名对应的属性为 `features`. `num_rows` 属性代表着数据集中样本的数量.

查看第一条样本:

```python
squad_it_dataset["train"][0]
```

```python
{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si è verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}
```

### 为不同的划分指定不同的本地文件

上面过程单独读取了 SQuAD-it 数据集的训练集. 但在训练的 pipeline 中, 我们往往还需要加载验证集. 我们希望通过 `load_dataset` 读取得到的 `DatasetDict` 结果中既包含 `train` 划分, 又包含 `test` 划分, 这样我们在对数据集进行处理时, 可以将两个不同的划分同时处理掉.

为了达到这个目标, 给 `data_files` 参数提供一个字典, 字典的 key 为划分名字, value 为划分集在本地的文件地址:

```python
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset
```

结果如下:

```python
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

### 读取压缩文件

```shell
wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```

上面的例子中, 我们通过下载下来数据集的压缩文件, 手动解压缩, 然后通过 `load_dataset` 函数加载.

Datasets 实际上提供了对数据文件自动解压缩的功能. 因此我们可以跳过自己手动解压缩的步骤, 直接将压缩文件用同样的方法指定, 给`load_dataset` 函数加载.

```python
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

在一些由大量的压缩文件组成的数据集情景下, 特别实用. 支持 `GZIP`, `ZIP`, `TAR` 这些常见的压缩打包类型的自动解压缩.

### 数据集划分由多个文件组成

有的数据集文件由多个文件组成, 我们可以将这多个本地文件名构成一个列表, 传递给 `data_files` 参数, 一次读取多个文件.

```python
data_files = ['train-001-of-003.json', 'train-002-of-003.json', 'train-003-of-003.json']
dataset = load_dataset('json', data_files=data_files)
```

读取了 `001 ~ 003` 三个文件, 加载为 `train` 划分.

如果我们的训练集包含3个文件, 测试集包含2个文件, 可以为不同的划分单独指定其对应的文件列表, 实现一次读取:

```python
data_files = {
    'train': ['train-001-of-003.json', 'train-002-of-003.json', 'train-003-of-003.json'],
    'test': ['test-001-of-002.json', 'test-002-of-002.json'],
}
dataset = load_dataset('json', data_files=data_files)
```

### 通过文件名模式匹配读取多个文件

有的数据集中包含特别多的文件, 我们可以通过字符串通配符匹配的方法, 一次性读取所有匹配到的文件.

例如我们可以读取指定目录下的所有 `JSON` 文件作为数据集.

```python
dataset = load_dataset('json', data_files='*.json')
```

再比如某数据集的训练集由1024个文件组成, 测试集由两个文件组成, 可以通过以下的方法一次性读取:

```python
data_files = {
    'train': 'train.0000*-of-01024.json',
    'test': ['test-001-of-002.json', 'test-002-of-002.json'],
}
dataset = load_dataset('json', data_files=data_files)
```

---

# 参考资料

- [Document: Load](https://huggingface.co/docs/datasets/main/en/loading)
- [What if my dataset isn't on the Hub?](https://huggingface.co/learn/nlp-course/chapter5/2?fw=pt)
