# Rearranging dataset

重新布置数据集, 指的是对数据集进行筛选样本, 划分数据集(创建 splits), 将大数据集分片等操作, 不会影响每一条样本中的内容. 接下来介绍一些常用的方法.

## Shuffle

### Dataset.shuffle()

**Dataset** 进行 `Dataset.shuffle()` 操作, 会在整个数据集上进行 shuffle. 它的原理是对 `[0, 1, 2, ... len(my_dataset) - 1]` 索引列表进行 shuffle, 然后在获取数据时, 先取打乱后的索引, 然后根据索引获取到相应的数据.

```python
my_dataset = my_dataset.shuffle(seed=42)
print(my_dataset[0])
```

实际上, shuffle 操作创建了一个 `indices mapping` 来完成上面的逻辑. 但是, 一旦一个数据集有了 `indices mapping`, 在这个数据集上的其他操作可能会慢10倍, 一个原因是因为拿数据的过程多了一个获取样本行的 index 步骤; 另外一个影响更大的原因是, 这种读取方式破坏了从连续的 chunk 中读取数据的高效性.

为了在 shuffle 之后的处理速度能够恢复的之前的程度, 需要将整个数据集在磁盘上重写一遍, 使打乱后的数据重新形成一块连续的 chunks, 并同时抛弃掉了 `indices mapping`.

```python
new_dataset = my_dataset.flatten_indices()
```

另外还有一种方法可以避免 shuffle 后效率的损失: 将 Dataset 转换为 IterableDataset, 然后使用 IterableDataset 的 shuffle 逻辑. 避免了对操作对全局样本应用, 而只是对 buffer 内的样本进行 shuffle.

```python
iterable_dataset = dataset.to_iterable_dataset(num_shards=128)
shuffled_iterable_dataset = iterable_dataset.shuffle(seed=42, buffer_size=1000)
```

### IterableDataset.shuffle()

**IterableDataset** 由于 lazy 机制, 无法提前获取到具体的样本, 也无法在文件中随机地获取样本, 因此无法使用与 `Dataset` 一样的方法对数据进行 shuffle. 所以 IterableDataset 无法对全部数据进行全局的 shuffle. `IterableDataset.shuffle()` 实现的是一种快速近似 shuffle, 通过维护一个 buffer 记录一定容量的样本, 然后在迭代过程中, 随机地从 buffer 中采样数据样本输出. 在使用 `shuffle()` 时, 需要指定 `buffer_size`, 默认为 1000:

```python
my_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)
for example in my_iterable_dataset:
    print(example)
    break
```

如果数据集是由多个文件组成, 这里每个文件中的数据对应着一个 **`shard`**. `IterableDataset.shuffle()` 还对 `shard` 维度进行 shuffle, 即 shuffle 后读取的前两条数据可能来自不同的文件.

## Sort

与打乱对应的就是排序. 根据指定列中不同样本对应的值得大小进行排序.

```python
dataset["label"][:10]
# [1, 0, 1, 0, 1, 1, 0, 1, 0, 0]
sorted_dataset = dataset.sort("label")
sorted_dataset["label"][:10]
# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
sorted_dataset["label"][-10:]
# [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

与 shuffle 类似, sort 也会创建 `indices mapping`, 而不是真正地改变底层数据. 后续找指定行的数据时, 再根据 `indices mapping` 找到对应行的 index.

## Select 切片

`select()` 会根据指定的索引, 切分出一个子数据集, 返回的也是一个 `Dataset` 对象.

```python
from datasets import load_dataset
ds = load_dataset("rotten_tomatoes", split="validation")
ds.select(range(4))
```

```python
Dataset({
    features: ['text', 'label'],
    num_rows: 4
})
```

与直接使用 `dataset[:k]` 不同的是, 后者直接切片返回的是字典, key 为数据集中的列, value 为每列的前 k 个值.

```python
from datasets import load_dataset

dataset = load_dataset('fka/awesome-chatgpt-prompts')
sub = dataset['train'][:2]
print(sub)
```

```python
{
    'act': [
        'Linux Terminal',
        'English Translator and Improver'
    ],
    'prompt': [
        'I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd',
        'I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. My first sentence is "istanbulu cok seviyom burada olmak cok guzel"'
    ]
}
```

## Filter

对行维度(样本)进行过滤, 满足符合要求的样本. 我们需要构造一个接受一条样本作为输入的函数, 根据样本中的信息判断是否采用这条样本.

```python
start_with_ar = dataset.filter(lambda example: example["sentence1"].startswith("Ar"))
len(start_with_ar)
start_with_ar["sentence1"]
```

另外, 还可以将样本的**索引**也考虑进来, 通过设置参数 `with_indices = True`.

```python
even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
len(even_dataset)
# 1834
len(dataset) / 2
# 1834.0
```

这个技巧在奇数/偶数索引代表不同类型的样本(例如0为问题, 1为答案)的情景中会用到.

## Split 划分

可以通过 [`train_test_split()`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) 函数将当前的 dataset 划分为 `train` 和 `test` 两个数据集. 可以通过 `test_size` 参数指定 `test` 数据集的样本数量或者比例. 当任务是**分类任务**, 还可以使用 `stratify_by_column` 参数指定标签列, 在每一类中都按指定的比例划分, 防止出现划分得到的测试集样本不平衡的问题.

```python
from datasets import load_dataset
ds = load_dataset("rotten_tomatoes", split="validation")
ds = ds.train_test_split(test_size=0.2, shuffle=True)

"""
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 852
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 214
    })
})
"""

ds = ds.train_test_split(test_size=0.2, seed=42)

ds = load_dataset("imdb",split="train")

"""
Dataset({
    features: ['text', 'label'],
    num_rows: 25000
})
"""

ds = ds.train_test_split(test_size=0.2, stratify_by_column="label")

"""
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 20000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 5000
    })
})
"""
```

## Shard

Datasets 可以将一个大规模数据集切分为若干个 chunks / shards. 可以通过 `shard()` 函数指定 `dataset.num_shards` 参数, 指定将数据集分为几个 shards.

```python
from datasets import load_dataset
ds = load_dataset("rotten_tomatoes", split="validation")

"""
Dataset({
    features: ['text', 'label'],
    num_rows: 1066
})
"""

ds.shard(num_shards=2, index=0)

"""
Dataset({
    features: ['text', 'label'],
    num_rows: 533
})
"""
```

`shard()` 方法会将一个空间连续的数据集, 切分为指定数量的多个连续 shards. 在调用 `shard()` 前, 要先使用 `shuffle()` 函数做打乱. 如果在切分之后再打乱, 就不再是全局上的随机打乱.

## Concatenate

### `axis = 0`

如果两个数据集, 有相同的 columns, 可以使用 `concatenate_datasets()` 函数将两个数据集合并, 形成一个新的数据集.

**可以用在 `IterableDataset` 类型的数据集中**.

```python
from datasets import concatenate_datasets, load_dataset

bookcorpus = load_dataset("bookcorpus", split="train")
wiki = load_dataset("wikipedia", "20220301.en", split="train")
wiki = wiki.remove_columns([col for col in wiki.column_names if col != "text"])  # only keep the 'text' column

assert bookcorpus.features.type == wiki.features.type
bert_dataset = concatenate_datasets([bookcorpus, wiki])
```

### `axis = 1`

如果两个数据集的样本数量为1, 并且相同索引的样本代表着一个样本的不同信息, 可以在列的维度合并, 得到的数据集每条样本包含两个数据集的 columns.

```python
from datasets import Dataset
bookcorpus_ids = Dataset.from_dict({"ids": list(range(len(bookcorpus)))})
bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=1)
```

## Interleave

Concatenate 是将两个数据集完整地合并到一起. 有时候我们想混合多个数据集, 但并不采用全部数据, 而是从每个数据集中采样一部分, 用这些部分样本合成为一个新数据集, 这种操作称为 **interleaving**. 通过 `interleave_datasets()` 函数实现, 可以指定每个数据集被采样的比例. 采样的过程是从原数据集中一条一条采样得到, 直到采集到指定数量, 或者原数据集耗尽.

```python
seed = 42
probabilities = [0.3, 0.5, 0.2]
d1 = Dataset.from_dict({"a": [0, 1, 2]})
d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
d3 = Dataset.from_dict({"a": [20, 21, 22]})
dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)
dataset["a"]
# [10, 11, 20, 12, 0, 21, 13]
```

可以通过 `stopping_strategy` 参数指定整个采样的停止过程.

- 默认为`first_exhausted`: 即新数据集的构建过程在遇到第一个子数据集耗尽的情况时就停止
- `all_exhausted`: 将会指定过采样(oversampling), 只有所有子数据集都发生样本耗尽的情况才会停止. 对于样本数据相对较少的子数据集, 会发生过采样. 如果不指定每个数据集的被采样的概率, 则新数据集的样本总量为 `max_length_datasets * nb_dataset`

```python
d1 = Dataset.from_dict({"a": [0, 1, 2]})
d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
d3 = Dataset.from_dict({"a": [20, 21, 22]})
dataset = interleave_datasets([d1, d2, d3], stopping_strategy="all_exhausted")
dataset["a"]
# [0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 20]
```

### 流式 Dataset 的 Interleave

**Interleave 可以用在 `IterableDataset` 类型的数据集中**.

可以将两个或多个 `IterableDataset` 组合成一个新的数据集. 新数据集也是 `IterableDataset` 类型的数据集, 在取样本时, 会交替地返回两个数据集中的样本.

```python
from datasets import interleave_datasets
en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)

multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
list(multilingual_dataset.take(2))
"""
[
    {'text': 'Mtendere Village was inspired by the vision...'},
    {'text': "Média de débat d'idées, de culture et de littérature..."}
]
"""
```

可以指定每个数据集被采样的概率. 下面的例子中, 目标是合成一个多语言数据集, 其中80%的样本为英语, 20%的样本为法语:

```python
multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)
list(multilingual_dataset_with_oversampling.take(2))
"""
[
    {'text': 'Mtendere Village was inspired by the vision...'},
    {'text': 'Lily James cannot fight the music...'}
]
"""
```

同样也可以使用 `stopping_strategy` 参数来指定全局采样停止的条件, 使用方法与 `Dataset` 相同.

---

# Adjusting dataset's metadata

## Rename

通过 `rename_column()` 方法, 可以将列名更改. 旧名称将会删除, 对应的数据会移动到新列名下.

```python
dataset
"""
Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx'],
    num_rows: 3668
})
"""

dataset = dataset.rename_column("sentence1", "sentenceA")
dataset = dataset.rename_column("sentence2", "sentenceB")
dataset
"""
Dataset({
    features: ['sentenceA', 'sentenceB', 'label', 'idx'],
    num_rows: 3668
})
"""
```

## Remove

`remove_columns()` 方法会将指定的列(包括名称和对应的数据)删除, 整个过程是 `IN-PLACE` 的.

```python
dataset = dataset.remove_columns("label")
dataset
"""
Dataset({
    features: ['sentence1', 'sentence2', 'idx'],
    num_rows: 3668
})
"""

dataset = dataset.remove_columns(["sentence1", "sentence2"])
dataset
"""
Dataset({
    features: ['idx'],
    num_rows: 3668
})
"""
```

`select_columns()` 与 `remove_columns()` 方法对应, 将会保存下来指定的列, 其他列将会被删除.

```python
dataset
"""
Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx'],
    num_rows: 3668
})
"""

dataset = dataset.select_columns(['sentence1', 'sentence2', 'idx'])
dataset
"""
Dataset({
    features: ['sentence1', 'sentence2', 'idx'],
    num_rows: 3668
})
"""

dataset = dataset.select_columns('idx')
dataset
"""
Dataset({
    features: ['idx'],
    num_rows: 3668
})
"""
```

## Cast

`cast()` 方法可以将多个列的 `feature type` 进行修改. 如果只想修改一个列, 可以使用 `cast_column()` 方法.

```python
dataset.features
"""
{
    'sentence1': Value(dtype='string', id=None),
    'sentence2': Value(dtype='string', id=None),
    'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
    'idx': Value(dtype='int32', id=None)
}
"""

from datasets import ClassLabel, Value
new_features = dataset.features.copy()
new_features["label"] = ClassLabel(names=["negative", "positive"])
new_features["idx"] = Value("int64")
dataset = dataset.cast(new_features)
dataset.features
"""
{
    'sentence1': Value(dtype='string', id=None),
    'sentence2': Value(dtype='string', id=None),
    'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None),
    'idx': Value(dtype='int64', id=None)
}
"""
```

```python
dataset.features
# {'audio': Audio(sampling_rate=44100, mono=True, id=None)}

dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
dataset.features
# {'audio': Audio(sampling_rate=16000, mono=True, id=None)}
```

## Flatten

有时一个列可能是 nested 列, 即包含多个属性. 例如 SQuAD 数据集中 `answers` 一列包含 `text` 和 `answer_start` 两个子属性, 可以通过 `flatten()` 方法将 subfields 提取出来.

```python
from datasets import load_dataset
dataset = load_dataset("squad", split="train")
dataset.features
"""
{
    'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),
    'context': Value(dtype='string', id=None),
    'id': Value(dtype='string', id=None),
    'question': Value(dtype='string', id=None),
    'title': Value(dtype='string', id=None)
}
"""

flat_dataset = dataset.flatten()
flat_dataset
"""
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],
 num_rows: 87599
}
"""
```

注意到, 碾平后, 原列消失, 新列名以原列名为前缀.

---

# Map

## with_indices

可以将样本的**索引**也考虑进来, 通过设置参数 `with_indices = True`.

```python
updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, with_indices=True)
updated_dataset["sentence2"][:5]
"""
['0: Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
 "1: Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .",
 "2: On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .",
 '3: Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',
 '4: PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .'
]
"""
```

**这个技巧在奇数/偶数索引代表不同类型的样本(例如0为问题, 1为答案)的情景中会用到.**

## Multiprocessing

通过在 CPU 启动多个进程并行处理, 提升整体的速度. `num_proc` 参数表示使用多少个进程去处理.

```python
updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, num_proc=4)
```

## Batched

**batch mapping 的一个目标是提升数据处理的速度, 往往使用 batch 处理数据要比一条条处理更快.**

将 `batched` 参数设置为 `True`, `map()` 方法可以批量地处理数据. 默认的 `batch_size` 为 1000.

可以通过 `map()` 方法改变样本的总数量. 例如将长文本按一定的窗口大小, 划分为多条短文本, 作为新样本.

```python
def chunk_examples(examples):
    chunks = []
    for sentence in examples["sentence1"]:
        chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]
    return {"chunks": chunks}

chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names)
dataset
"""
Dataset({
 features: ['sentence1', 'sentence2', 'label', 'idx'],
 num_rows: 3668
})
"""

chunked_dataset
"""
Dataset(schema: {'chunks': 'string'}, num_rows: 10470)
"""
```

因此可以看出使用 `map()` 函数, 不要求输入和输出的 size 必须相等. 但是在样本有其他列的情况下, 如果返回的样本数量与原本的不同, 将会产生对其的问题, 所以会报错. 可以使用 `remove_columns` 参数指定移除其他列, 避免出现这个问题.

```python
from datasets import Dataset
dataset = Dataset.from_dict({"a": [0, 1, 2]})
dataset.map(lambda batch: {"b": batch["a"] * 2}, batched=True)  # new column with 6 elements: [0, 1, 2, 0, 1, 2]
# ArrowInvalid: Column 1 named b expected length 3 but got length 6

from datasets import Dataset
dataset = Dataset.from_dict({"a": [0, 1, 2]})
dataset_with_duplicates = dataset.map(lambda batch: {"b": batch["a"] * 2}, remove_columns=["a"], batched=True)
len(dataset_with_duplicates)
# 6
```

---

# 参考资料

- [Process](https://huggingface.co/docs/datasets/main/en/process#process)
- [Batch mapping](https://huggingface.co/docs/datasets/main/en/about_map_batch)
