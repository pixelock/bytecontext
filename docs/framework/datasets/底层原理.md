# Apache Arrow

[Apache Arrow](https://arrow.apache.org/) 是一种特别的数据格式, 帮助我们对大规模数据集也可以高效地进行移动和处理. 它有以下的优点:

- 采用了**零拷贝**(zero-copy)读取技术, 消除了**序列化**的开销
- 它是与编程语言无关的, 支持不同的编程语言
- Arrow是面向列的, 因此查询和处理数据片或数据列的速度更快
- Arrow允许无复制切换到标准机器学习工具, 如NumPy, Pandas, PyTorch和TensorFlow
- Arrow支持嵌套的列类型

# 内存映射

HF Datasets 借助 Arrow 作为 **local caching system**. 这是一种由**磁盘缓存**(on-disk cache)为数据提供支持的技术, 磁盘缓存再由内存提供映射(memory-mapped), 实现快速定位查找.

这种架构使得在小内存机器上, 就能对大数据集进行加载. **Arrow data 并不会直接保存在内存中, 而是保存在磁盘中, 内存中保存的是映射表, 提供在磁盘中获取数据的通道.** 并借助虚拟内存实现快速查找.

```python
import os; import psutil; import timeit
from datasets import load_dataset

# Process.memory_info is expressed in bytes, so convert to megabytes 
mem_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
wiki = load_dataset("wikipedia", "20220301.en", split="train")
mem_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)

print(f"RAM memory used: {(mem_after - mem_before)} MB")
# RAM memory used: 50 MB
```

上面的例子中加载的是完整的英文维基百科数据集, 却只占用了 `50 MB` 的内存.

# 缓存

缓存是 Datasets 表现高效的一个重要原因. Datasets 缓存了下载的数据集以及处理得到的数据集, 当在此用到这些数据时, 会重新从缓存中加载, 而不是重新下载或者再次运行处理函数得到结果.

## Fingerprint

缓存的一个关键问题是**如何记录哪些处理逻辑应用到了数据集上**. Datasets 通过 **fingerprint** 机制实现. fingerprint 对数据集的当前状态进行跟踪记录. 初始的 fingerprint 是通过对 Arrow table(数据集来自内存, 如pandas DataFrame) 或者 Arrow files(数据集在磁盘中) 进行 hash compute 得到的. 后续的 fingerprints 由 **前序的 fingerprints** 以及在**数据集上的最后一步转换对应的 hash 值** 综合计算得到. 这里说的转换包括[对数据集进行重新布置](/docs/framework/datasets/对数据集进行重新布置.md)中提到的所有操作.

fingerprint 实际中为:

```python
from datasets import Dataset
dataset1 = Dataset.from_dict({"a": [0, 1, 2]})
dataset2 = dataset1.map(lambda x: {"a": x["a"] + 1})
print(dataset1._fingerprint, dataset2._fingerprint)
# d19493523d95e2dc 5b86abacd4b42434
```

转换操作必须是可哈希的, 它可以被 dill 或者 pickle 序列化. 如果使用了一个不可哈希的转换操作, Datasets 并不会报错, 而是生成一个随机的 fingerprints, 并会抛出一个 warning. 但由于中间有 fingerprints 的计算中断了(由前序+转换操作hash, 变成了随机), 付出的代价是再次使用这个数据时, 会重新对所有的转换计算一遍. 这将会是非常耗时的. 因此一定要保证转换函数可以被序列化.

缓存机制是可以关闭的. 关闭之后, 在每次启动 Python session 处理数据时, 都会在临时目录中创建缓存文件, 当 Python session 关闭时, 临时目录中的缓存文件就会被删除. 因此当缓存机制关闭时, 在处理完数据之后, 要用 `Dataset.save_to_disk()` 将处理后的数据保存到本地磁盘, 否则所有的处理都会在进程结束时被删除.

# Hashing

数据集的 fingerprint 的更新会根据处理函数的 hashing, 而 hashing 是根据传入到 `map` 方法中的处理函数, 以及其 `batch_size`, `remove_columns` 等参数共同得到的.

函数的 hashing 可以通过下面的方法得到:

```python
from datasets.fingerprint import Hasher
my_func = lambda example: {"length": len(example["text"])}
print(Hasher.hash(my_func))
# '3d35e2b3e94c81d6'
```

hashing 具体是通过使用 `dill pickler` 将对象序列化为 bytes, 然后再对序列化得到的 bytes 进行哈希计算得到. `dill pickler` 递归地将函数中使用到的参数序列化, 所以函数调用中任何细节的变动, 都会影响最后的哈希值.

当不同的 Python session 调用时同一个 function 有不同的哈希值, 说明至少有一个参数包含不确定(not deterministic)的因素(比如是动态得到的值). 需要定位到这个函数, 并解决掉这个问题.

---

# 参考资料

- [Datasets 🤝 Arrow](https://huggingface.co/docs/datasets/main/en/about_arrow)
- [The cache](https://huggingface.co/docs/datasets/main/en/about_cache)
