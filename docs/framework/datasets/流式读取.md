# Dataset vs IterableDataset

在 Datasets 中包含两种数据集类型:

- 一种是 [Dataset](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset). 它由 `Apache Arrow table` 作为驱动. 通常用来处理一些读取后可以全部放在内容中不发生 OOM 的数据集
- 另一种是 [IterableDataset](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset), 一般用来处理超大数据集, 例如几百 GBs, 磁盘中放不下

**IterableDataset** 是处理超大型数据集的方式. 通过懒加载的方式, 控制使用的资源规模.

通过 `load_dataset` 读取本地或远程文件:

```python
data_files = {"train": ["path/to/data.csv"]}
my_dataset = load_dataset("csv", data_files=data_files, split="train")
my_dataset
```

返回的是 Dataset 类型的数据集. 这里有一步**将 CSV 文件转换成 Arrow 格式, 并保存到磁盘中**, 当文件特别大时, 将会非常耗时, 甚至超过磁盘的容量上限.

为了节省磁盘空间, 以及跳过格式转换这一步节省时间, 我们使用流式读取. 只需要将 `load_dataset` 的参数 `streaming` 指定为 `True`:

```python
data_files = {"train": ["path/to/data.csv"]}
my_iterable_dataset = load_dataset("csv", data_files=data_files, split="train", streaming=True)
my_iterable_dataset
```

返回的是 IterableDataset 类型的数据集, 然后使用对数据集迭代的方式使用. 使用流式读取就这么简单.

# 流式读取的优点

总结下, 流式读取有以下优点, 或者说适用的情况

- 不需要等待将一个特别大的数据集完全下载下来
- 不需要等待将全部数据转换成 Arrow 格式, 在大数据集中这一步会特别耗时
- 转换后的数据文件大小超过了磁盘大小, 全部读取不可行
- 快速地探索下数据集中的样本长什么样子, 而不希望全部读取

例如对于 [oscar-corpus/OSCAR-2201](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201) 数据集, 有 1.2TB 大小. 我们可以通过以下的方式读取其中的几百个 `JSONL` 文件:

```python
from datasets import load_dataset
data_files = {'train': 'path/to/OSCAR-2201/compressed/en_meta/*.jsonl.gz'}
dataset = load_dataset('json', data_files=data_files, split='train', streaming=True)
print(next(iter(dataset)))
```

输出:

```
{'id': 0, 'text': 'Founded in 2015, Golden Bees is a leading programmatic recruitment platform dedicated to employers, HR agencies and job boards. The company has developed unique HR-custom technologies and predictive algorithms to identify and attract the best candidates for a job opportunity.', ...
```

上面是将文件下载到本地读取的. 因为文件太大, 下载的过程也会消耗很多时间, 我们可以直接读取线上的数据:

```python
from datasets import load_dataset
dataset = load_dataset('oscar-corpus/OSCAR-2201', 'en', split='train', streaming=True)
print(next(iter(dataset)))
```

# `map()` 方法在 Dataset / IterableDataset 中的不同表现

**Dataset** 对象在使用 `Dataset.map()` 对数据进行处理时, 会对整个数据集立即进行处理, 并返回处理结果.

**IterableDataset** 在调用 `IterableDataset.map()` 方法时, 并不会立即开始对数据集中的样本进行处理, **只有在真正读取到这条数据时, 才会开始处理**. 也就是说, 当我们在对 `IterableDataset` 数据集进行迭代时, 才会真正应用 `map` 方法中处理数据的函数:

```python
my_iterable_dataset = my_iterable_dataset.map(process_fn_1)
my_iterable_dataset = my_iterable_dataset.filter(filter_fn)
my_iterable_dataset = my_iterable_dataset.map(process_fn_2)

# process_fn_1, filter_fn and process_fn_2 are applied on-the-fly when iterating over the dataset
for example in my_iterable_dataset:  
    print(example)
    break
```

# IterableDataset 的 shuffling

**Dataset** 进行 `Dataset.shuffle()` 操作, 会在整个数据集上进行 shuffle. 它的原理是对 `[0, 1, 2, ... len(my_dataset) - 1]` 索引列表进行 shuffle, 然后在获取数据时, 先取打乱后的索引, 然后根据索引获取到相应的数据.

```python
my_dataset = my_dataset.shuffle(seed=42)
print(my_dataset[0])
```

**IterableDataset** 由于 lazy 机制, 无法提前获取到具体的样本, 也无法在文件中随机地获取样本, 因此无法使用与 `Dataset` 一样的方法对数据进行 shuffle. 所以 IterableDataset 无法对全部数据进行全局的 shuffle. `IterableDataset.shuffle()` 实现的是一种快速近似 shuffle, 通过维护一个 buffer 记录一定容量的样本, 然后在迭代过程中, 随机地从 buffer 中采样数据样本输出. 在使用 `shuffle()` 时, 需要指定 `buffer_size`, 默认为 1000:

```python
my_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)
for example in my_iterable_dataset:
    print(example)
    break
```

如果数据集是由多个文件组成, 这里每个文件中的数据对应着一个 **`shard`**. `IterableDataset.shuffle()` 还对 `shard` 维度进行 shuffle, 即 shuffle 后读取的前两条数据可能来自不同的文件.

## Reshuffle

训练的过程中, 如果想在一个 epoch 结束后, 对数据集重新进行打乱, 方法是**在每个 epoch 开始时, 给数据集设定不同的 seed**. 对于 IterableDataset, 可以使用 `IterableDataset.set_epoch(epoch)` 实现这一步, 调用后数据集的 seed 变为 `initial seed + current epoch`:

```python
for epoch in range(epochs):
    shuffled_dataset.set_epoch(epoch)
    for example in shuffled_dataset:
        ...
```

# 划分数据集

对于 `IterableDataset`, 想要在数据集中划分出训练集和测试集, 需要使用 `IterableDataset.take()` 和 `IterableDataset.skip()` 两个方法配合实现.

```python
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10000)
test_head = shuffled_dataset.take(1000)
train_dataset = shuffled_dataset.skip(1000)
```

通过 `IterableDataset.take(n)` 获取前 `n` 条样本, 作为测试集. 然后使用 `IterableDataset.skip(n)` 跳过前 `n` 条样本, 将剩余的样本作为新的数据集返回, 作为训练集.

**注意**: 在调用 `take()` 和 `skip()` 之前, 需要先调用 `shuffle()`. 因为在调用了`take()` 或 `skip()` 之后, 数据集的 `shards` 顺序会锁定住, 不能再进行 shuffle 了.
