# 一维卷积

## 一维卷积的各种类型

一维卷积通常有三种类型:

- Full
- same
- valid

下面以一个长度为5的一维张量I和长度为3的一维张量K作为卷积核为例, 介绍这三种卷积的计算过程.

![](/resources/images/framework/pytorch/conv1d-1.png)

### Full Conv

Full卷积的计算过程是, 卷积核K的最后一位`1`首先对其待卷积的I的第一位`3`, K沿着I顺序移动, 每移动到一个固定位置, 对应位置的值相乘再求和:

![](/resources/images/framework/pytorch/conv1d-2.png)

最后的结果为:

![](/resources/images/framework/pytorch/conv1d-3.png)

结果张量的长度是长于原始张量`I`的长度的, 头尾都长出了对应的padding长度. 在这个例子中, pandding的长度为2.

### Same Conv

卷积核K都有一个**锚点**. 无论卷积核长度`S`为奇数还是偶数, 锚点的位置在`S // 2`处.

![](/resources/images/framework/pytorch/conv1d-6.png)

首先将锚点与I的第一位`3`对齐, 然后将锚点顺序移动到张量I的每一个位置处, 对应位置相乘再求和:

![](/resources/images/framework/pytorch/conv1d-4.png)

最后的结果为:

![](/resources/images/framework/pytorch/conv1d-5.png)

可以看到结果张量的长度与原始张量`I`的长度一致, 这也是`Same`的来源.

### Valid Conv

**Valid卷积只考虑I能完全覆盖K的情况**, 即K在I的内部移动, 完全无padding的情况, 计算过程如下:

![](/resources/images/framework/pytorch/conv1d-7.png)

最后的结果为:

![](/resources/images/framework/pytorch/conv1d-8.png)

Valid卷积会使得结果张量的长度缩小.

### 三种卷积类型的关系

![](/resources/images/framework/pytorch/conv1d-9.png)

## 带有深度/隐向量的一维卷积

上面的例子中, `I`作为输入矩阵, 矩阵的长度为5, 深度或者说每个位置代表的隐向量长度为1. 在NLP中, 每个token往往是由一个隐向量表示, 记隐向量的长度为`hidden_size`, 例如在BERT中, 每个输入到Transformer Block中的token都是由一个长度为768的隐向量表示.

在一维卷积中, 对带有深度的张量做卷积, 需要对应的卷积核也具有相同的深度. 比如输入张量`x`为一个长度为3, `hidden_size`为3的张量, 对应的卷积核`K`长度为2, `hidden_size`也为3, 以same卷积为例, 计算过程如下:

![](/resources/images/framework/pytorch/conv1d-10.png)

## 带有深度的张量与多个卷积核的一维卷积

至此为止, 以上全部例子都是使用了一个卷积核, 得到的结果张量的深度都为1. 如果需要输出张量的深度大于1, 就需要使用多个卷积核, 每个卷积核各自输出一个深度维度, 然后在深度维度的方向上连接起来.

同一个张量与多个卷积核的卷积本质上是该张量分别与每一个卷积核卷积, 然后将每一个卷积结果在深度方向上连接起来.

以长度为3, 深度为3, 与两个长度为2, 深度也为3的卷积核卷积为例, 计算过程如下:

![](/resources/images/framework/pytorch/conv1d-11.png)

---

# nn.Conv1d

torch中的一位卷积层定义如下, 我们对它的关键参数进行解析.

```python
class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
```

- in_channels. 输入张量的通道, 在NLP中, 即token embedding vector的长度
- out_channels. 卷积产生的通道. 有多少个out_channels, 就需要多少个1维卷积核
- kernel_size(int). 卷积核的长度`k`

这三个参数定义完, 这一层的卷积核规模也就确定了: 每个卷积核的大小为`k * in_channels`, 共有`out_channels`个卷积核.

接下来的函数是定义卷积的行为:

- stride. 卷积步长, 默认为1, 即每次在序列维度上移动一位
- padding. 前后各pad多个个0. 默认为0, 即valid卷积

然后是理解起来比较复杂的参数`groups`, 它代表了**从输入通道到输出通道的阻塞连接数**. 

groups会把输入的channel分成几个group, 假设我们有一个长度为7, channel为2的1D输入:

![](/resources/images/framework/pytorch/conv1d-12.png)

那么如果nn.Conv1d参数设置为`in_channels=2, out_channels=2, kernel_size=3, group=1`, 这就是最普通的卷积:

![](/resources/images/framework/pytorch/conv1d-13.png)

Conv1d_layer.weight的形状为`(2,2,3)`, 表示需要2个filter(对应out_channels), 每个filter覆盖2个channel(对应in_channels), 长度为3. 或者可以直接理解为需要2个形状为`(2,3)`的filter.

如果nn.Conv1d参数是`in_channels=2, out_channels=2, kernel_size=3, group=2`:

![](/resources/images/framework/pytorch/conv1d-14.png)

Conv1d_layer.weight的形状为`(2,1,3)`, 表示需要2个filter(对应out_channels), 每个filter覆盖1个channel(对应in_channels), 长度为3. 或者可以直接理解为需要2个形状为`(1,3)`的filter.

将nn.Conv1d的参数设置为`in_channels=2, out_channels=4, kernel_size=3, group=2`, 这样的话就会给每个group初始化两个filter, 2个group就会输出总共4个channel.

![](/resources/images/framework/pytorch/conv1d-15.png)

Conv1d_layer.weight的形状为`(4,1,3)`, 表示需要4个filter, 每个filter覆盖1个channel, 长度为3. 或者可以直接理解为需要4个形状为(1,3)的filter.

对比可以看到, 在设定了`groups`之后, 对于输入张量, 在`in_channels`方向, 会将其输入深度划分成指定数量的组. 在`out_channels`方向, 也会将对应的卷积核分划成指定数量的组, 每个组处理对应的输入深度, 各组之间互不影响.

---

# 参考资料

- [CONV1D](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)
- [各种一维卷积（Full卷积、Same卷积、Valid卷积、带深度的一维卷积）](https://blog.csdn.net/u013602059/article/details/104947737)
- [记录一下torch.nn.Conv1d的group参数](https://zhuanlan.zhihu.com/p/616003771)
- [pytorch之nn.Conv1d详解](https://blog.csdn.net/sunny_xsc1994/article/details/82969867)
