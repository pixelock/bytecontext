# T5的网络结构

T5使用了标准的Encoder-Decoder模型, Encoder和Decoder都是用了多层Transformer组成. Base版本的模型使用的是12层12head的注意力机制, 与BERT类似, 总参数量为1.1亿, 约为BERT的两倍.

模型的一些独特之处在于:

## 相对位置编码

T5中的位置信息, 与BERT通过Position Embedding将位置信息以输入的形式传入不同, T5使用相对位置, 将两两token之间的相对位置信息直接在attention矩阵中体现.

具体来说, 在tokens之间的Q, K内积得到logit矩阵之后, 在两两token的attention score之上, 根据它们之间的相对位置关系, 施加对应的标量影响, 再计算softmax得到attention矩阵. 做法是通过桶函数$$\text{b}(t-s)$$, 将相对距离$$t-s$$从`[-128, 128]`的幅度压缩至`[0, 31]`. 每个桶在每个head上对应一个**可训练**的**标量**参数.

- 在同一层中, 每个head由自己单独的relative position bias参数
- relative position bias分别加在Encoder和Decoder的第一层, 其他层没有. 两部分使用的参数不是共享的.
- 相对位置超过128的两个token, 在计算时做截断, 以128或-128处理
- 桶函数是**非线性**的, 函数遵循比较邻近的位置需要分的比较得精细一些, 所以给它们都分配一个独立的位置编码; 稍远的位置, 不用区分得太清楚, 所以它们可以共用一个位置编码, 距离越远, 共用的范围就可以越大, 直到达到指定范围再截断


```python
# encoder或decoder的__init__部分
self.block = nn.ModuleList(
    [T5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]
)  # 只在第一层有relative position bias

# 模型整体__init__部分
self.encoder = T5Stack(encoder_config, self.shared)
self.decoder = T5Stack(decoder_config, self.shared)
```

**参考资料**

- [让研究人员绞尽脑汁的Transformer位置编码](https://spaces.ac.cn/archives/8130#T5%E5%BC%8F)
- [Huggingface Code](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py#L388)

---

# 预训练任务

T5的理念就是将所有文本任务都转换成seq2seq任务解决. 预训练任务要围绕这个目标进行.

T5的预训练包含**无监督**和**有监督**两部分.

## 无监督任务

与BERT的思想类似, 将文本遮盖住一部分作为输入, 输出补全被遮盖的这部分. 为了适应Seq2Seq这种形式, 输入输出的形式变成:

> **输入**: 明月几时有，[M0]问青天，不知[M1]，今夕是何年？我欲[M2]归去，又恐琼楼玉宇，高处[M3]；起舞[M4]清影，何似在人间。

> **输出**: [M0]把酒[M1]天上宫阙[M2]乘风[M3]不胜寒[M4]弄

对不同位置的文本片段使用不同的掩码标记, 在输出端不对原始句子进行完全重构, 而是重构丢弃的文本片段, 并通过掩码标记指示恢复片段的位置信息.

## 有监督任务

收集了常见的NLP监督任务数据, 并也统一转化为SeqSeq任务来训练. 转换的方式为, 根据不同类型的任务, 在原输入前加上对应适合这个任务的Prompt, 输出则为原本label的文本形式的表达(如分类任务输出对应的标签). 在[Huggingface](https://huggingface.co/t5-base/blob/main/config.json)版本的T5中, 根据配置文件, 可以看到使用了4中有监督预训练任务:

```json
{
    "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  }
}
```

## 小结

有监督和无监督任务的训练样本, 是混合在一起进行的. 样本构建类似于:

```python
# Unsupervised denoising training
input_ids = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="pt").input_ids
labels = tokenizer("<extra_id_0> cute dog <extra_id_1> the <extra_id_2>", return_tensors="pt").input_ids
# the forward function automatically creates the correct decoder_input_ids
loss = model(input_ids=input_ids, labels=labels).loss
loss.item()
```

```python
# Supervised training: seq2seq
input_ids = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt").input_ids
labels = tokenizer("Das Haus ist wunderbar.", return_tensors="pt").input_ids
# the forward function automatically creates the correct decoder_input_ids
loss = model(input_ids=input_ids, labels=labels).loss
loss.item()
```

**参考资料**

- [Huggingface Code](https://huggingface.co/t5-base/blob/main/config.json)
- [T5 Tutorial](https://github.com/chunhuizhang/bert_t5_gpt/blob/main/tutorials/09_t5_overall.ipynb)
- [T5模型简单介绍](https://blog.csdn.net/weixin_45684362/article/details/130216187)
- [那个屠榜的T5模型，现在可以在中文上玩玩了](https://kexue.fm/archives/7867)

---

# T5.1.1

T5之后, Google还做了一次升级, 将升级前的T5模型称为T5.1.0, 升级后的为T5.1.1. 升级的内容主要有:

## Relative Position Bias 应用层变化

在T5中, 只有encoder和decoder的第一层transformer是使用了relative_position_bias, 其他层没有位置信息的加入.

在T5.1.1中, encoder和decoder的所有层都加入了relative_position_bias.

## Feed-forward结构激活函数替换

T5的FFN可以表示为:

$$
\text{FFN}(x)=\text{relu}(xW_1)W_2
$$

T5.1.1中的FFN将中间的激活函数由ReLU替换为GLU(Gated Linear Unit):

$$
\text{FFN}_{\text{GEGLU}}(x)=\big(\text{gelu}(xW_1)\otimes xW_2\big)W_3
$$

FFN层增加了50%参数, 在论文中效果提升明显.

## Embedding共享机制改变

在T5中, Encoder和Decoder的Embedding层, Decoder最后预测概率分布的Softmax层**三者是共享同一个Embedding矩阵的**. 现在T5.1.1只让Encoder和Decoder的Embedding层共享, Decoder最后预测概率分布的Softmax层则用了一个独立的Embedding矩阵.

这会让参数量大大增加, 论文中的实验证明, 会带来一定的提升.

## 预训练值保留无监督训练

T5.1.1只做了无监督预训练, 但效果依然相当出色.

## 小结

由于T5.1.1的出色表现, mT5(Multilingual T5, T5的多语言版)使用了T5.1.1的结构, 这里名称可能会带来一定的混淆.

---
